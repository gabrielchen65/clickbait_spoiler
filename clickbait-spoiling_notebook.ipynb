{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6194838,"sourceType":"datasetVersion","datasetId":3556201},{"sourceId":6195509,"sourceType":"datasetVersion","datasetId":3556591},{"sourceId":6205384,"sourceType":"datasetVersion","datasetId":3563055},{"sourceId":6231902,"sourceType":"datasetVersion","datasetId":3579852},{"sourceId":6233162,"sourceType":"datasetVersion","datasetId":3580680},{"sourceId":6238800,"sourceType":"datasetVersion","datasetId":3584317},{"sourceId":6244823,"sourceType":"datasetVersion","datasetId":3588317},{"sourceId":7803154,"sourceType":"datasetVersion","datasetId":4569318},{"sourceId":7828050,"sourceType":"datasetVersion","datasetId":4587388},{"sourceId":15572,"sourceType":"modelInstanceVersion","modelInstanceId":12961},{"sourceId":15812,"sourceType":"modelInstanceVersion","modelInstanceId":13166},{"sourceId":15893,"sourceType":"modelInstanceVersion","modelInstanceId":13237}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Question-Answering Section","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/gabrielchen65/clickbait_spoiler.git\n!pip install -q accelerate >= 0.12.0\n!pip install -q datasets >= 1.8.0\n!pip install -q torch >= 1.3.0\n!pip install -q evaluate\n!pip install -q peft\n!pip install -U nltk","metadata":{"execution":{"iopub.status.busy":"2024-03-14T19:46:17.685993Z","iopub.execute_input":"2024-03-14T19:46:17.686612Z","iopub.status.idle":"2024-03-14T19:47:07.373272Z","shell.execute_reply.started":"2024-03-14T19:46:17.686584Z","shell.execute_reply":"2024-03-14T19:47:07.372194Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'clickbait_spoiler'...\nremote: Enumerating objects: 167, done.\u001b[K\nremote: Counting objects: 100% (167/167), done.\u001b[K\nremote: Compressing objects: 100% (115/115), done.\u001b[K\nremote: Total 167 (delta 62), reused 151 (delta 46), pack-reused 0\u001b[K\nReceiving objects: 100% (167/167), 10.29 MiB | 8.16 MiB/s, done.\nResolving deltas: 100% (62/62), done.\n\u001b[31mERROR: Could not find a version that satisfies the requirement 0.12.0 (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for 0.12.0\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement 1.8.0 (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for 1.8.0\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement 1.3.0 (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for 1.3.0\u001b[0m\u001b[31m\n\u001b[0mRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nCollecting nltk\n  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.7)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.3.2)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk) (2023.12.25)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.66.1)\nDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nltk\n  Attempting uninstall: nltk\n    Found existing installation: nltk 3.2.4\n    Uninstalling nltk-3.2.4:\n      Successfully uninstalled nltk-3.2.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.8.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nltk-3.8.1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Delete the folder if want to retrain with the setting","metadata":{}},{"cell_type":"code","source":"!rm -r ./tmp/roberta-large-lora-best-40","metadata":{"execution":{"iopub.status.busy":"2024-03-14T19:50:24.148050Z","iopub.execute_input":"2024-03-14T19:50:24.148844Z","iopub.status.idle":"2024-03-14T19:50:25.097418Z","shell.execute_reply.started":"2024-03-14T19:50:24.148806Z","shell.execute_reply":"2024-03-14T19:50:25.096282Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"rm: cannot remove './tmp/roberta-large-lora-best-40': No such file or directory\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Training","metadata":{}},{"cell_type":"code","source":"## training script\nimport os; os.environ[\"WANDB_DISABLED\"] = \"true\"\n!python3 /kaggle/working/clickbait_spoiler/task2/question-answering/run_qa.py \\\n  --model_name_or_path deepset/roberta-large-squad2 \\\n  --train_file /kaggle/input/question-answering-dataset/train.json \\\n  --validation_file /kaggle/input/question-answering-dataset/val-nonmulti-concat.json \\\n  --do_train \\\n  --do_eval \\\n  --per_device_train_batch_size 12 \\\n  --learning_rate 3e-4 \\\n  --num_train_epochs 15 \\\n  --max_seq_length 386 \\\n  --doc_stride 128 \\\n  --output_dir ./tmp/roberta-large-lora-best-15 \\\n  --training_w_peft True \\\n  --save_strategy no \\\n  --save_total_limit 1 \\\n  --load_best_model_at_end True \\\n\n#   --save_strategy epoch \\\n#   --training_w_peft True \\\n#   --evaluation_strategy epoch\n#   --version_2_with_negative","metadata":{"execution":{"iopub.status.busy":"2024-03-14T19:51:54.591903Z","iopub.execute_input":"2024-03-14T19:51:54.592866Z","iopub.status.idle":"2024-03-14T23:27:10.297095Z","shell.execute_reply.started":"2024-03-14T19:51:54.592826Z","shell.execute_reply":"2024-03-14T23:27:10.295927Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"2024-03-14 19:51:59.563609: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-14 19:51:59.563666: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-14 19:51:59.565257: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 605.54it/s]\n[INFO|configuration_utils.py:728] 2024-03-14 19:52:04,464 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--deepset--roberta-large-squad2/snapshots/c419f18ae4147c99b4198cee58ac3317e127477d/config.json\n[INFO|configuration_utils.py:791] 2024-03-14 19:52:04,469 >> Model config RobertaConfig {\n  \"_name_or_path\": \"deepset/roberta-large-squad2\",\n  \"architectures\": [\n    \"RobertaForQuestionAnswering\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 1024,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"language\": \"english\",\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 514,\n  \"model_type\": \"roberta\",\n  \"name\": \"Roberta\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"pad_token_id\": 1,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.38.1\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}\n\n[INFO|configuration_utils.py:728] 2024-03-14 19:52:04,574 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--deepset--roberta-large-squad2/snapshots/c419f18ae4147c99b4198cee58ac3317e127477d/config.json\n[INFO|configuration_utils.py:791] 2024-03-14 19:52:04,576 >> Model config RobertaConfig {\n  \"_name_or_path\": \"deepset/roberta-large-squad2\",\n  \"architectures\": [\n    \"RobertaForQuestionAnswering\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 1024,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"language\": \"english\",\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 514,\n  \"model_type\": \"roberta\",\n  \"name\": \"Roberta\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"pad_token_id\": 1,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.38.1\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}\n\n[INFO|tokenization_utils_base.py:2046] 2024-03-14 19:52:04,577 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--deepset--roberta-large-squad2/snapshots/c419f18ae4147c99b4198cee58ac3317e127477d/vocab.json\n[INFO|tokenization_utils_base.py:2046] 2024-03-14 19:52:04,578 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--deepset--roberta-large-squad2/snapshots/c419f18ae4147c99b4198cee58ac3317e127477d/merges.txt\n[INFO|tokenization_utils_base.py:2046] 2024-03-14 19:52:04,578 >> loading file tokenizer.json from cache at None\n[INFO|tokenization_utils_base.py:2046] 2024-03-14 19:52:04,578 >> loading file added_tokens.json from cache at None\n[INFO|tokenization_utils_base.py:2046] 2024-03-14 19:52:04,578 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--deepset--roberta-large-squad2/snapshots/c419f18ae4147c99b4198cee58ac3317e127477d/special_tokens_map.json\n[INFO|tokenization_utils_base.py:2046] 2024-03-14 19:52:04,578 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--deepset--roberta-large-squad2/snapshots/c419f18ae4147c99b4198cee58ac3317e127477d/tokenizer_config.json\n[INFO|configuration_utils.py:728] 2024-03-14 19:52:04,578 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--deepset--roberta-large-squad2/snapshots/c419f18ae4147c99b4198cee58ac3317e127477d/config.json\n[INFO|configuration_utils.py:791] 2024-03-14 19:52:04,579 >> Model config RobertaConfig {\n  \"_name_or_path\": \"deepset/roberta-large-squad2\",\n  \"architectures\": [\n    \"RobertaForQuestionAnswering\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 1024,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"language\": \"english\",\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 514,\n  \"model_type\": \"roberta\",\n  \"name\": \"Roberta\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"pad_token_id\": 1,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.38.1\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}\n\n[INFO|configuration_utils.py:728] 2024-03-14 19:52:04,666 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--deepset--roberta-large-squad2/snapshots/c419f18ae4147c99b4198cee58ac3317e127477d/config.json\n[INFO|configuration_utils.py:791] 2024-03-14 19:52:04,668 >> Model config RobertaConfig {\n  \"_name_or_path\": \"deepset/roberta-large-squad2\",\n  \"architectures\": [\n    \"RobertaForQuestionAnswering\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 1024,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"language\": \"english\",\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 514,\n  \"model_type\": \"roberta\",\n  \"name\": \"Roberta\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"pad_token_id\": 1,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.38.1\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}\n\n[INFO|modeling_utils.py:3257] 2024-03-14 19:52:04,754 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--deepset--roberta-large-squad2/snapshots/c419f18ae4147c99b4198cee58ac3317e127477d/model.safetensors\n[INFO|modeling_utils.py:3992] 2024-03-14 19:52:05,431 >> All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n\n[INFO|modeling_utils.py:4000] 2024-03-14 19:52:05,431 >> All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at deepset/roberta-large-squad2.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n============================== Using peft (LoRA) ==============================\ntrainable params: 198,658 || all params: 354,510,852 || trainable%: 0.05603721264927597\n==========================================================================================\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n[INFO|trainer.py:1812] 2024-03-14 19:52:07,411 >> ***** Running training *****\n[INFO|trainer.py:1813] 2024-03-14 19:52:07,411 >>   Num examples = 8,948\n[INFO|trainer.py:1814] 2024-03-14 19:52:07,411 >>   Num Epochs = 15\n[INFO|trainer.py:1815] 2024-03-14 19:52:07,411 >>   Instantaneous batch size per device = 12\n[INFO|trainer.py:1818] 2024-03-14 19:52:07,411 >>   Total train batch size (w. parallel, distributed & accumulation) = 12\n[INFO|trainer.py:1819] 2024-03-14 19:52:07,411 >>   Gradient Accumulation steps = 1\n[INFO|trainer.py:1820] 2024-03-14 19:52:07,411 >>   Total optimization steps = 11,190\n[INFO|trainer.py:1821] 2024-03-14 19:52:07,415 >>   Number of trainable parameters = 198,658\n{'loss': 1.3222, 'grad_norm': 7.058314800262451, 'learning_rate': 0.00028659517426273456, 'epoch': 0.67}\n{'loss': 1.2088, 'grad_norm': 11.239047050476074, 'learning_rate': 0.00027319034852546915, 'epoch': 1.34}\n{'loss': 1.1302, 'grad_norm': 14.52817153930664, 'learning_rate': 0.00025978552278820374, 'epoch': 2.01}\n{'loss': 1.0306, 'grad_norm': 9.849257469177246, 'learning_rate': 0.00024638069705093833, 'epoch': 2.68}\n{'loss': 1.0161, 'grad_norm': 6.382290363311768, 'learning_rate': 0.00023297587131367292, 'epoch': 3.35}\n{'loss': 0.9596, 'grad_norm': 19.26153564453125, 'learning_rate': 0.0002195710455764075, 'epoch': 4.02}\n{'loss': 0.9001, 'grad_norm': 12.705751419067383, 'learning_rate': 0.0002061662198391421, 'epoch': 4.69}\n{'loss': 0.862, 'grad_norm': 14.050968170166016, 'learning_rate': 0.00019276139410187665, 'epoch': 5.36}\n{'loss': 0.8631, 'grad_norm': 9.596894264221191, 'learning_rate': 0.00017935656836461124, 'epoch': 6.03}\n{'loss': 0.781, 'grad_norm': 14.693216323852539, 'learning_rate': 0.00016595174262734583, 'epoch': 6.7}\n{'loss': 0.7674, 'grad_norm': 17.045385360717773, 'learning_rate': 0.0001525469168900804, 'epoch': 7.37}\n{'loss': 0.7273, 'grad_norm': 14.917196273803711, 'learning_rate': 0.00013914209115281498, 'epoch': 8.04}\n{'loss': 0.6848, 'grad_norm': 20.9516658782959, 'learning_rate': 0.00012573726541554957, 'epoch': 8.71}\n{'loss': 0.6335, 'grad_norm': 27.622787475585938, 'learning_rate': 0.00011233243967828417, 'epoch': 9.38}\n{'loss': 0.6519, 'grad_norm': 15.49038314819336, 'learning_rate': 9.892761394101876e-05, 'epoch': 10.05}\n{'loss': 0.5904, 'grad_norm': 18.347885131835938, 'learning_rate': 8.552278820375335e-05, 'epoch': 10.72}\n{'loss': 0.6043, 'grad_norm': 17.02230453491211, 'learning_rate': 7.211796246648794e-05, 'epoch': 11.39}\n{'loss': 0.5475, 'grad_norm': 9.609904289245605, 'learning_rate': 5.871313672922251e-05, 'epoch': 12.06}\n{'loss': 0.5246, 'grad_norm': 25.695064544677734, 'learning_rate': 4.53083109919571e-05, 'epoch': 12.73}\n{'loss': 0.5306, 'grad_norm': 10.106371879577637, 'learning_rate': 3.190348525469169e-05, 'epoch': 13.4}\n{'loss': 0.5273, 'grad_norm': 63.2680778503418, 'learning_rate': 1.8498659517426272e-05, 'epoch': 14.08}\n{'loss': 0.5042, 'grad_norm': 12.44125747680664, 'learning_rate': 5.093833780160857e-06, 'epoch': 14.75}\n100%|███████████████████████████████████| 11190/11190 [3:34:16<00:00,  1.03s/it][INFO|trainer.py:2067] 2024-03-14 23:26:23,992 >> \n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n{'train_runtime': 12856.5779, 'train_samples_per_second': 10.44, 'train_steps_per_second': 0.87, 'train_loss': 0.7839830911617602, 'epoch': 15.0}\n100%|███████████████████████████████████| 11190/11190 [3:34:16<00:00,  1.15s/it]\n[INFO|trainer.py:3067] 2024-03-14 23:26:23,995 >> Saving model checkpoint to ./tmp/roberta-large-lora-best-15\n[INFO|configuration_utils.py:728] 2024-03-14 23:26:24,402 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--deepset--roberta-large-squad2/snapshots/c419f18ae4147c99b4198cee58ac3317e127477d/config.json\n[INFO|configuration_utils.py:791] 2024-03-14 23:26:24,404 >> Model config RobertaConfig {\n  \"_name_or_path\": \"../saved_models/farm_roberta-large-squad2/language_model.bin\",\n  \"architectures\": [\n    \"RobertaForQuestionAnswering\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 1024,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"language\": \"english\",\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 514,\n  \"model_type\": \"roberta\",\n  \"name\": \"Roberta\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"pad_token_id\": 1,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.38.1\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}\n\n[INFO|tokenization_utils_base.py:2459] 2024-03-14 23:26:24,415 >> tokenizer config file saved in ./tmp/roberta-large-lora-best-15/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2468] 2024-03-14 23:26:24,416 >> Special tokens file saved in ./tmp/roberta-large-lora-best-15/special_tokens_map.json\n***** train metrics *****\n  epoch                    =       15.0\n  train_loss               =      0.784\n  train_runtime            = 3:34:16.57\n  train_samples            =       8948\n  train_samples_per_second =      10.44\n  train_steps_per_second   =       0.87\n[INFO|trainer.py:759] 2024-03-14 23:26:24,473 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `PeftModelForQuestionAnswering.forward`,  you can safely ignore this message.\n[INFO|trainer.py:3376] 2024-03-14 23:26:24,475 >> ***** Running Evaluation *****\n[INFO|trainer.py:3378] 2024-03-14 23:26:24,475 >>   Num examples = 829\n[INFO|trainer.py:3381] 2024-03-14 23:26:24,475 >>   Batch size = 8\n100%|█████████████████████████████████████████| 104/104 [00:34<00:00,  3.28it/s]\n  0%|                                                   | 0/316 [00:00<?, ?it/s]\u001b[A\n  3%|█▍                                        | 11/316 [00:00<00:03, 95.34it/s]\u001b[A\n  7%|██▉                                       | 22/316 [00:00<00:03, 96.13it/s]\u001b[A\n 10%|████▎                                     | 32/316 [00:00<00:02, 97.76it/s]\u001b[A\n 13%|█████▌                                    | 42/316 [00:00<00:02, 94.66it/s]\u001b[A\n 16%|██████▉                                   | 52/316 [00:00<00:02, 93.25it/s]\u001b[A\n 20%|████████▏                                 | 62/316 [00:00<00:02, 94.60it/s]\u001b[A\n 23%|█████████▌                                | 72/316 [00:00<00:02, 91.73it/s]\u001b[A\n 26%|██████████▉                               | 82/316 [00:00<00:02, 88.00it/s]\u001b[A\n 29%|████████████                              | 91/316 [00:01<00:02, 86.41it/s]\u001b[A\n 32%|█████████████                            | 101/316 [00:01<00:02, 89.86it/s]\u001b[A\n 35%|██████████████▍                          | 111/316 [00:01<00:02, 78.95it/s]\u001b[A\n 39%|███████████████▊                         | 122/316 [00:01<00:02, 85.87it/s]\u001b[A\n 42%|█████████████████▎                       | 133/316 [00:01<00:02, 85.21it/s]\u001b[A\n 46%|██████████████████▋                      | 144/316 [00:01<00:02, 83.93it/s]\u001b[A\n 49%|████████████████████                     | 155/316 [00:01<00:01, 84.95it/s]\u001b[A\n 52%|█████████████████████▎                   | 164/316 [00:01<00:01, 79.31it/s]\u001b[A\n 55%|██████████████████████▍                  | 173/316 [00:02<00:02, 68.48it/s]\u001b[A\n 59%|████████████████████████                 | 185/316 [00:02<00:01, 75.29it/s]\u001b[A\n 61%|█████████████████████████▏               | 194/316 [00:02<00:01, 72.69it/s]\u001b[A\n 64%|██████████████████████████▎              | 203/316 [00:02<00:01, 74.81it/s]\u001b[A\n 68%|███████████████████████████▉             | 215/316 [00:02<00:01, 84.44it/s]\u001b[A\n 71%|█████████████████████████████            | 224/316 [00:02<00:01, 83.49it/s]\u001b[A\n 74%|██████████████████████████████▎          | 234/316 [00:02<00:00, 82.13it/s]\u001b[A\n 78%|███████████████████████████████▉         | 246/316 [00:02<00:00, 88.98it/s]\u001b[A\n 81%|█████████████████████████████████▏       | 256/316 [00:03<00:00, 88.56it/s]\u001b[A\n 84%|██████████████████████████████████▍      | 265/316 [00:03<00:00, 84.84it/s]\u001b[A\n 87%|███████████████████████████████████▌     | 274/316 [00:03<00:00, 81.34it/s]\u001b[A\n 91%|█████████████████████████████████████▎   | 288/316 [00:03<00:00, 96.43it/s]\u001b[A\n 94%|██████████████████████████████████████▋  | 298/316 [00:03<00:00, 62.77it/s]\u001b[A\n100%|█████████████████████████████████████████| 316/316 [00:03<00:00, 82.06it/s]\u001b[A\n100%|█████████████████████████████████████████| 104/104 [00:42<00:00,  2.43it/s]\n***** eval metrics *****\n  epoch                   =       15.0\n  eval_meteor             =     0.4147\n  eval_runtime            = 0:00:35.03\n  eval_samples            =        829\n  eval_samples_per_second =     23.663\n  eval_steps_per_second   =      2.969\n[INFO|modelcard.py:450] 2024-03-14 23:27:07,729 >> Dropping the following result as it does not have all the necessary fields:\n{'task': {'name': 'Question Answering', 'type': 'question-answering'}}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### compress the folder and download the model","metadata":{}},{"cell_type":"code","source":"!zip -r roberta-large.zip /kaggle/working/tmp/roberta-large","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create a link to download the .zip file","metadata":{}},{"cell_type":"code","source":"!zip -r roberta-large.zip /kaggle/working/tmp/roberta-large\n\nfrom IPython.display import FileLink \nFileLink(r'roberta-large.zip')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation for QA\nmemory usage with lora, roberta large:\ntraining: 13 GB\neval: 1.9 GB","metadata":{}},{"cell_type":"code","source":"import os; os.environ[\"WANDB_DISABLED\"] = \"true\"\n# original_model_name_for_peft_eval: for loading the model after fine-tuning with LoRA\n!python3 /kaggle/working/clickbait_spoiler/task2/question-answering/run_qa.py \\\n  --model_name_or_path /kaggle/input/qa_model/pytorch/roberta-large-lora-2238-epoch3-meteor0.43/1/roberta-large/checkpoint-2238 \\\n  --validation_file /kaggle/input/question-answering-dataset/val-nonmulti-concat.json \\\n  --do_eval \\\n  --evaluation_metric meteor \\\n  --per_device_train_batch_size 12 \\\n  --max_seq_length 386 \\\n  --doc_stride 128 \\\n  --output_dir ./tmp/pilot_test/ \\\n  --peft_eval True\n# !python3 /kaggle/working/clickbait_spoiler/task2/question-answering/run_qa.py \\\n#   --model_name_or_path /kaggle/input/qa_model/pytorch/roberta-large-lora-2238-epoch3-meteor0.43/1/roberta-large/checkpoint-2238 \\\n#   --validation_file /kaggle/input/question-answering-dataset/val-nonmulti-concat.json \\\n#   --do_eval \\\n#   --evaluation_metric meteor \\\n#   --per_device_train_batch_size 12 \\\n#   --max_seq_length 386 \\\n#   --doc_stride 128 \\\n#   --output_dir ./tmp/test/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Summarization Section","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/gabrielchen65/clickbait_spoiler.git\n!pip install -q accelerate >= 0.12.0\n!pip install -q datasets >= 1.8.0\n!pip install -q sentencepiece != 0.1.92\n!pip install -q protobuf\n!pip install -q rouge-score\n!pip install -U nltk\n!pip install -q py7zr\n!pip install -q torch >= 1.3\n!pip install -q evaluate","metadata":{"execution":{"iopub.status.busy":"2024-03-13T15:35:50.587160Z","iopub.execute_input":"2024-03-13T15:35:50.587987Z","iopub.status.idle":"2024-03-13T15:37:22.474163Z","shell.execute_reply.started":"2024-03-13T15:35:50.587945Z","shell.execute_reply":"2024-03-13T15:37:22.472865Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'clickbait_spoiler'...\nremote: Enumerating objects: 167, done.\u001b[K\nremote: Counting objects: 100% (167/167), done.\u001b[K\nremote: Compressing objects: 100% (115/115), done.\u001b[K\nremote: Total 167 (delta 62), reused 151 (delta 46), pack-reused 0\u001b[K\nReceiving objects: 100% (167/167), 10.29 MiB | 16.36 MiB/s, done.\nResolving deltas: 100% (62/62), done.\n\u001b[31mERROR: Could not find a version that satisfies the requirement 0.12.0 (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for 0.12.0\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement 1.8.0 (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for 1.8.0\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: Invalid requirement: '!='\u001b[0m\u001b[31m\n\u001b[0mRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nCollecting nltk\n  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.7)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.3.2)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk) (2023.12.25)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.66.1)\nDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nltk\n  Attempting uninstall: nltk\n    Found existing installation: nltk 3.2.4\n    Uninstalling nltk-3.2.4:\n      Successfully uninstalled nltk-3.2.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.8.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nltk-3.8.1\n\u001b[31mERROR: Could not find a version that satisfies the requirement 1.3 (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for 1.3\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -q peft","metadata":{"execution":{"iopub.status.busy":"2024-03-13T15:46:23.381236Z","iopub.execute_input":"2024-03-13T15:46:23.381601Z","iopub.status.idle":"2024-03-13T15:46:36.553603Z","shell.execute_reply.started":"2024-03-13T15:46:23.381569Z","shell.execute_reply":"2024-03-13T15:46:36.552410Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!rm -rf ./tmp/non-multi-bart-large-best-40-lora","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os; os.environ[\"WANDB_DISABLED\"] = \"true\"\n!python3 /kaggle/working/clickbait_spoiler/task2/summarization/run_summarization.py \\\n    --model_name_or_path facebook/bart-large-cnn \\\n    --do_train \\\n    --do_eval \\\n    --text_column article \\\n    --summary_column highlights \\\n    --train_file /kaggle/input/clickbait-spoiling-summarization-dataset/train_4summary_multi-concat.json \\\n    --validation_file /kaggle/input/clickbait-spoiling-summarization-dataset/val_4summary_multi-concat.json \\\n    --dataset_config \"3.0.0\" \\\n    --per_device_train_batch_size=4 \\\n    --per_device_eval_batch_size=4 \\\n    --predict_with_generate \\\n    --num_train_epochs 40 \\\n    --output_dir ./tmp/non-multi-bart-large-best-40-lora \\\n    --max_target_length 60 \\\n    --save_strategy no \\\n    --save_total_limit 1 \\\n    --load_best_model_at_end True \\\n    --training_w_peft True \\\n    --lora_rank 4\n    # lora_alpha 16\n    #--evaluation_strategy epoch\n    #    --predict_with_generate \\\n    #    --source_prefix \"summarize: \" \\ only t5 model\n    #    --save_total_limit 5","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-03-13T15:46:44.967580Z","iopub.execute_input":"2024-03-13T15:46:44.967972Z","iopub.status.idle":"2024-03-13T17:02:52.751059Z","shell.execute_reply.started":"2024-03-13T15:46:44.967935Z","shell.execute_reply":"2024-03-13T17:02:52.749805Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"2024-03-13 15:46:53.725638: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-13 15:46:53.725785: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-13 15:46:53.845497: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nDownloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-3875a9baf432e37d/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\nDownloading data files: 100%|███████████████████| 2/2 [00:00<00:00, 8019.70it/s]\nExtracting data files: 100%|█████████████████████| 2/2 [00:00<00:00, 186.23it/s]\nDataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-3875a9baf432e37d/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\n100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 324.22it/s]\nconfig.json: 100%|█████████████████████████| 1.58k/1.58k [00:00<00:00, 6.17MB/s]\n[INFO|configuration_utils.py:728] 2024-03-13 15:47:06,912 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/37f520fa929c961707657b28798b30c003dd100b/config.json\n[INFO|configuration_utils.py:791] 2024-03-13 15:47:06,921 >> Model config BartConfig {\n  \"_name_or_path\": \"facebook/bart-large-cnn\",\n  \"_num_labels\": 3,\n  \"activation_dropout\": 0.0,\n  \"activation_function\": \"gelu\",\n  \"add_final_layer_norm\": false,\n  \"architectures\": [\n    \"BartForConditionalGeneration\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 0,\n  \"classif_dropout\": 0.0,\n  \"classifier_dropout\": 0.0,\n  \"d_model\": 1024,\n  \"decoder_attention_heads\": 16,\n  \"decoder_ffn_dim\": 4096,\n  \"decoder_layerdrop\": 0.0,\n  \"decoder_layers\": 12,\n  \"decoder_start_token_id\": 2,\n  \"dropout\": 0.1,\n  \"early_stopping\": true,\n  \"encoder_attention_heads\": 16,\n  \"encoder_ffn_dim\": 4096,\n  \"encoder_layerdrop\": 0.0,\n  \"encoder_layers\": 12,\n  \"eos_token_id\": 2,\n  \"force_bos_token_to_be_generated\": true,\n  \"forced_bos_token_id\": 0,\n  \"forced_eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\"\n  },\n  \"init_std\": 0.02,\n  \"is_encoder_decoder\": true,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2\n  },\n  \"length_penalty\": 2.0,\n  \"max_length\": 142,\n  \"max_position_embeddings\": 1024,\n  \"min_length\": 56,\n  \"model_type\": \"bart\",\n  \"no_repeat_ngram_size\": 3,\n  \"normalize_before\": false,\n  \"num_beams\": 4,\n  \"num_hidden_layers\": 12,\n  \"output_past\": true,\n  \"pad_token_id\": 1,\n  \"prefix\": \" \",\n  \"scale_embedding\": false,\n  \"task_specific_params\": {\n    \"summarization\": {\n      \"early_stopping\": true,\n      \"length_penalty\": 2.0,\n      \"max_length\": 142,\n      \"min_length\": 56,\n      \"no_repeat_ngram_size\": 3,\n      \"num_beams\": 4\n    }\n  },\n  \"transformers_version\": \"4.38.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 50264\n}\n\n[INFO|tokenization_auto.py:617] 2024-03-13 15:47:07,143 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n[INFO|configuration_utils.py:728] 2024-03-13 15:47:07,362 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/37f520fa929c961707657b28798b30c003dd100b/config.json\n[INFO|configuration_utils.py:791] 2024-03-13 15:47:07,363 >> Model config BartConfig {\n  \"_name_or_path\": \"facebook/bart-large-cnn\",\n  \"_num_labels\": 3,\n  \"activation_dropout\": 0.0,\n  \"activation_function\": \"gelu\",\n  \"add_final_layer_norm\": false,\n  \"architectures\": [\n    \"BartForConditionalGeneration\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 0,\n  \"classif_dropout\": 0.0,\n  \"classifier_dropout\": 0.0,\n  \"d_model\": 1024,\n  \"decoder_attention_heads\": 16,\n  \"decoder_ffn_dim\": 4096,\n  \"decoder_layerdrop\": 0.0,\n  \"decoder_layers\": 12,\n  \"decoder_start_token_id\": 2,\n  \"dropout\": 0.1,\n  \"early_stopping\": true,\n  \"encoder_attention_heads\": 16,\n  \"encoder_ffn_dim\": 4096,\n  \"encoder_layerdrop\": 0.0,\n  \"encoder_layers\": 12,\n  \"eos_token_id\": 2,\n  \"force_bos_token_to_be_generated\": true,\n  \"forced_bos_token_id\": 0,\n  \"forced_eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\"\n  },\n  \"init_std\": 0.02,\n  \"is_encoder_decoder\": true,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2\n  },\n  \"length_penalty\": 2.0,\n  \"max_length\": 142,\n  \"max_position_embeddings\": 1024,\n  \"min_length\": 56,\n  \"model_type\": \"bart\",\n  \"no_repeat_ngram_size\": 3,\n  \"normalize_before\": false,\n  \"num_beams\": 4,\n  \"num_hidden_layers\": 12,\n  \"output_past\": true,\n  \"pad_token_id\": 1,\n  \"prefix\": \" \",\n  \"scale_embedding\": false,\n  \"task_specific_params\": {\n    \"summarization\": {\n      \"early_stopping\": true,\n      \"length_penalty\": 2.0,\n      \"max_length\": 142,\n      \"min_length\": 56,\n      \"no_repeat_ngram_size\": 3,\n      \"num_beams\": 4\n    }\n  },\n  \"transformers_version\": \"4.38.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 50264\n}\n\nvocab.json: 100%|████████████████████████████| 899k/899k [00:00<00:00, 1.13MB/s]\nmerges.txt: 100%|█████████████████████████████| 456k/456k [00:00<00:00, 760kB/s]\ntokenizer.json: 100%|██████████████████████| 1.36M/1.36M [00:00<00:00, 6.67MB/s]\n[INFO|tokenization_utils_base.py:2046] 2024-03-13 15:47:11,329 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/37f520fa929c961707657b28798b30c003dd100b/vocab.json\n[INFO|tokenization_utils_base.py:2046] 2024-03-13 15:47:11,329 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/37f520fa929c961707657b28798b30c003dd100b/merges.txt\n[INFO|tokenization_utils_base.py:2046] 2024-03-13 15:47:11,329 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/37f520fa929c961707657b28798b30c003dd100b/tokenizer.json\n[INFO|tokenization_utils_base.py:2046] 2024-03-13 15:47:11,329 >> loading file added_tokens.json from cache at None\n[INFO|tokenization_utils_base.py:2046] 2024-03-13 15:47:11,329 >> loading file special_tokens_map.json from cache at None\n[INFO|tokenization_utils_base.py:2046] 2024-03-13 15:47:11,329 >> loading file tokenizer_config.json from cache at None\n[INFO|configuration_utils.py:728] 2024-03-13 15:47:11,329 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/37f520fa929c961707657b28798b30c003dd100b/config.json\n[INFO|configuration_utils.py:791] 2024-03-13 15:47:11,331 >> Model config BartConfig {\n  \"_name_or_path\": \"facebook/bart-large-cnn\",\n  \"_num_labels\": 3,\n  \"activation_dropout\": 0.0,\n  \"activation_function\": \"gelu\",\n  \"add_final_layer_norm\": false,\n  \"architectures\": [\n    \"BartForConditionalGeneration\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 0,\n  \"classif_dropout\": 0.0,\n  \"classifier_dropout\": 0.0,\n  \"d_model\": 1024,\n  \"decoder_attention_heads\": 16,\n  \"decoder_ffn_dim\": 4096,\n  \"decoder_layerdrop\": 0.0,\n  \"decoder_layers\": 12,\n  \"decoder_start_token_id\": 2,\n  \"dropout\": 0.1,\n  \"early_stopping\": true,\n  \"encoder_attention_heads\": 16,\n  \"encoder_ffn_dim\": 4096,\n  \"encoder_layerdrop\": 0.0,\n  \"encoder_layers\": 12,\n  \"eos_token_id\": 2,\n  \"force_bos_token_to_be_generated\": true,\n  \"forced_bos_token_id\": 0,\n  \"forced_eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\"\n  },\n  \"init_std\": 0.02,\n  \"is_encoder_decoder\": true,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2\n  },\n  \"length_penalty\": 2.0,\n  \"max_length\": 142,\n  \"max_position_embeddings\": 1024,\n  \"min_length\": 56,\n  \"model_type\": \"bart\",\n  \"no_repeat_ngram_size\": 3,\n  \"normalize_before\": false,\n  \"num_beams\": 4,\n  \"num_hidden_layers\": 12,\n  \"output_past\": true,\n  \"pad_token_id\": 1,\n  \"prefix\": \" \",\n  \"scale_embedding\": false,\n  \"task_specific_params\": {\n    \"summarization\": {\n      \"early_stopping\": true,\n      \"length_penalty\": 2.0,\n      \"max_length\": 142,\n      \"min_length\": 56,\n      \"no_repeat_ngram_size\": 3,\n      \"num_beams\": 4\n    }\n  },\n  \"transformers_version\": \"4.38.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 50264\n}\n\nmodel.safetensors: 100%|███████████████████| 1.63G/1.63G [00:32<00:00, 50.3MB/s]\n[INFO|modeling_utils.py:3257] 2024-03-13 15:47:44,495 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/37f520fa929c961707657b28798b30c003dd100b/model.safetensors\n[INFO|configuration_utils.py:845] 2024-03-13 15:47:44,532 >> Generate config GenerationConfig {\n  \"bos_token_id\": 0,\n  \"decoder_start_token_id\": 2,\n  \"early_stopping\": true,\n  \"eos_token_id\": 2,\n  \"forced_bos_token_id\": 0,\n  \"forced_eos_token_id\": 2,\n  \"length_penalty\": 2.0,\n  \"max_length\": 142,\n  \"min_length\": 56,\n  \"no_repeat_ngram_size\": 3,\n  \"num_beams\": 4,\n  \"pad_token_id\": 1\n}\n\n[INFO|modeling_utils.py:3992] 2024-03-13 15:47:46,352 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n\n[INFO|modeling_utils.py:4000] 2024-03-13 15:47:46,352 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-large-cnn.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\ngeneration_config.json: 100%|██████████████████| 363/363 [00:00<00:00, 1.56MB/s]\n[INFO|configuration_utils.py:800] 2024-03-13 15:47:46,810 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/37f520fa929c961707657b28798b30c003dd100b/generation_config.json\n[INFO|configuration_utils.py:845] 2024-03-13 15:47:46,811 >> Generate config GenerationConfig {\n  \"bos_token_id\": 0,\n  \"decoder_start_token_id\": 2,\n  \"early_stopping\": true,\n  \"eos_token_id\": 2,\n  \"forced_bos_token_id\": 0,\n  \"forced_eos_token_id\": 2,\n  \"length_penalty\": 2.0,\n  \"max_length\": 142,\n  \"min_length\": 56,\n  \"no_repeat_ngram_size\": 3,\n  \"num_beams\": 4,\n  \"pad_token_id\": 1\n}\n\n============================== Using peft (LoRA) ==============================\ntrainable params: 589,824 || all params: 406,880,256 || trainable%: 0.1449625513409036\n==========================================================================================\n[INFO|modeling_utils.py:1875] 2024-03-13 15:47:47,040 >> You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 50265. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\nRunning tokenizer on train dataset: 100%|█████████| 1/1 [00:00<00:00,  1.01ba/s]\nRunning tokenizer on validation dataset: 100%|████| 1/1 [00:00<00:00,  6.61ba/s]\nDownloading builder script: 100%|██████████| 6.93k/6.93k [00:00<00:00, 15.9MB/s]\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[INFO|trainer.py:1812] 2024-03-13 15:47:56,559 >> ***** Running training *****\n[INFO|trainer.py:1813] 2024-03-13 15:47:56,559 >>   Num examples = 559\n[INFO|trainer.py:1814] 2024-03-13 15:47:56,559 >>   Num Epochs = 40\n[INFO|trainer.py:1815] 2024-03-13 15:47:56,559 >>   Instantaneous batch size per device = 4\n[INFO|trainer.py:1818] 2024-03-13 15:47:56,559 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n[INFO|trainer.py:1819] 2024-03-13 15:47:56,559 >>   Gradient Accumulation steps = 1\n[INFO|trainer.py:1820] 2024-03-13 15:47:56,559 >>   Total optimization steps = 5,600\n[INFO|trainer.py:1821] 2024-03-13 15:47:56,563 >>   Number of trainable parameters = 589,824\n{'loss': 0.9622, 'grad_norm': 1.5858044624328613, 'learning_rate': 4.5535714285714286e-05, 'epoch': 3.57}\n{'loss': 0.6505, 'grad_norm': 3.0889036655426025, 'learning_rate': 4.107142857142857e-05, 'epoch': 7.14}\n{'loss': 0.5812, 'grad_norm': 1.7505578994750977, 'learning_rate': 3.6607142857142853e-05, 'epoch': 10.71}\n{'loss': 0.5209, 'grad_norm': 4.117457866668701, 'learning_rate': 3.2142857142857144e-05, 'epoch': 14.29}\n{'loss': 0.4592, 'grad_norm': 8.363360404968262, 'learning_rate': 2.767857142857143e-05, 'epoch': 17.86}\n{'loss': 0.4466, 'grad_norm': 3.6312520503997803, 'learning_rate': 2.3214285714285715e-05, 'epoch': 21.43}\n{'loss': 0.4183, 'grad_norm': 4.717465400695801, 'learning_rate': 1.8750000000000002e-05, 'epoch': 25.0}\n{'loss': 0.3904, 'grad_norm': 2.219489574432373, 'learning_rate': 1.4285714285714285e-05, 'epoch': 28.57}\n{'loss': 0.3805, 'grad_norm': 5.276528835296631, 'learning_rate': 9.821428571428573e-06, 'epoch': 32.14}\n{'loss': 0.3811, 'grad_norm': 2.699915885925293, 'learning_rate': 5.357142857142857e-06, 'epoch': 35.71}\n{'loss': 0.3638, 'grad_norm': 4.890476703643799, 'learning_rate': 8.928571428571428e-07, 'epoch': 39.29}\n100%|█████████████████████████████████████| 5600/5600 [1:13:58<00:00,  1.31it/s][INFO|trainer.py:2067] 2024-03-13 17:01:54,679 >> \n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n{'train_runtime': 4438.1163, 'train_samples_per_second': 5.038, 'train_steps_per_second': 1.262, 'train_loss': 0.5023304898398263, 'epoch': 40.0}\n100%|█████████████████████████████████████| 5600/5600 [1:13:58<00:00,  1.26it/s]\n[INFO|trainer.py:3067] 2024-03-13 17:01:54,682 >> Saving model checkpoint to ./tmp/non-multi-bart-large-best-40-lora\n[INFO|configuration_utils.py:728] 2024-03-13 17:01:55,240 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-large-cnn/snapshots/37f520fa929c961707657b28798b30c003dd100b/config.json\n[INFO|configuration_utils.py:791] 2024-03-13 17:01:55,242 >> Model config BartConfig {\n  \"_num_labels\": 3,\n  \"activation_dropout\": 0.0,\n  \"activation_function\": \"gelu\",\n  \"add_final_layer_norm\": false,\n  \"architectures\": [\n    \"BartForConditionalGeneration\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 0,\n  \"classif_dropout\": 0.0,\n  \"classifier_dropout\": 0.0,\n  \"d_model\": 1024,\n  \"decoder_attention_heads\": 16,\n  \"decoder_ffn_dim\": 4096,\n  \"decoder_layerdrop\": 0.0,\n  \"decoder_layers\": 12,\n  \"decoder_start_token_id\": 2,\n  \"dropout\": 0.1,\n  \"early_stopping\": true,\n  \"encoder_attention_heads\": 16,\n  \"encoder_ffn_dim\": 4096,\n  \"encoder_layerdrop\": 0.0,\n  \"encoder_layers\": 12,\n  \"eos_token_id\": 2,\n  \"force_bos_token_to_be_generated\": true,\n  \"forced_bos_token_id\": 0,\n  \"forced_eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\"\n  },\n  \"init_std\": 0.02,\n  \"is_encoder_decoder\": true,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2\n  },\n  \"length_penalty\": 2.0,\n  \"max_length\": 142,\n  \"max_position_embeddings\": 1024,\n  \"min_length\": 56,\n  \"model_type\": \"bart\",\n  \"no_repeat_ngram_size\": 3,\n  \"normalize_before\": false,\n  \"num_beams\": 4,\n  \"num_hidden_layers\": 12,\n  \"output_past\": true,\n  \"pad_token_id\": 1,\n  \"prefix\": \" \",\n  \"scale_embedding\": false,\n  \"task_specific_params\": {\n    \"summarization\": {\n      \"early_stopping\": true,\n      \"length_penalty\": 2.0,\n      \"max_length\": 142,\n      \"min_length\": 56,\n      \"no_repeat_ngram_size\": 3,\n      \"num_beams\": 4\n    }\n  },\n  \"transformers_version\": \"4.38.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 50264\n}\n\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n[INFO|tokenization_utils_base.py:2459] 2024-03-13 17:01:56,127 >> tokenizer config file saved in ./tmp/non-multi-bart-large-best-40-lora/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2468] 2024-03-13 17:01:56,127 >> Special tokens file saved in ./tmp/non-multi-bart-large-best-40-lora/special_tokens_map.json\n***** train metrics *****\n  epoch                    =       40.0\n  train_loss               =     0.5023\n  train_runtime            = 1:13:58.11\n  train_samples            =        559\n  train_samples_per_second =      5.038\n  train_steps_per_second   =      1.262\n[INFO|trainer.py:3376] 2024-03-13 17:01:56,196 >> ***** Running Evaluation *****\n[INFO|trainer.py:3378] 2024-03-13 17:01:56,196 >>   Num examples = 84\n[INFO|trainer.py:3381] 2024-03-13 17:01:56,196 >>   Batch size = 4\n[INFO|configuration_utils.py:845] 2024-03-13 17:01:56,219 >> Generate config GenerationConfig {\n  \"bos_token_id\": 0,\n  \"decoder_start_token_id\": 2,\n  \"early_stopping\": true,\n  \"eos_token_id\": 2,\n  \"forced_bos_token_id\": 0,\n  \"forced_eos_token_id\": 2,\n  \"length_penalty\": 2.0,\n  \"max_length\": 142,\n  \"min_length\": 56,\n  \"no_repeat_ngram_size\": 3,\n  \"num_beams\": 4,\n  \"pad_token_id\": 1\n}\n\n100%|███████████████████████████████████████████| 21/21 [00:50<00:00,  2.40s/it]\n***** eval metrics *****\n  epoch                   =       40.0\n  eval_gen_len            =    59.5595\n  eval_loss               =      0.965\n  eval_meteor             =     51.628\n  eval_runtime            = 0:00:53.00\n  eval_samples            =         84\n  eval_samples_per_second =      1.585\n  eval_steps_per_second   =      0.396\n[INFO|modelcard.py:450] 2024-03-13 17:02:49,499 >> Dropping the following result as it does not have all the necessary fields:\n{'task': {'name': 'Summarization', 'type': 'summarization'}}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### packing and download the weight","metadata":{}},{"cell_type":"code","source":"!zip -r bart-large-best-40-lora.zip ./tmp/non-multi-bart-large-best-40-lora\n\nfrom IPython.display import FileLink \nFileLink(r'bart-large-best-40-lora.zip')","metadata":{"execution":{"iopub.status.busy":"2024-03-13T17:24:14.868410Z","iopub.execute_input":"2024-03-13T17:24:14.868798Z","iopub.status.idle":"2024-03-13T17:25:52.127878Z","shell.execute_reply.started":"2024-03-13T17:24:14.868761Z","shell.execute_reply":"2024-03-13T17:25:52.126852Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"  adding: tmp/non-multi-bart-large-best-40-lora/ (stored 0%)\n  adding: tmp/non-multi-bart-large-best-40-lora/README.md (deflated 47%)\n  adding: tmp/non-multi-bart-large-best-40-lora/train_results.json (deflated 41%)\n  adding: tmp/non-multi-bart-large-best-40-lora/vocab.json (deflated 59%)\n  adding: tmp/non-multi-bart-large-best-40-lora/all_results.json (deflated 56%)\n  adding: tmp/non-multi-bart-large-best-40-lora/adapter_model.safetensors (deflated 41%)\n  adding: tmp/non-multi-bart-large-best-40-lora/adapter_config.json (deflated 51%)\n  adding: tmp/non-multi-bart-large-best-40-lora/trainer_state.json (deflated 71%)\n  adding: tmp/non-multi-bart-large-best-40-lora/runs/ (stored 0%)\n  adding: tmp/non-multi-bart-large-best-40-lora/runs/Mar13_15-47-05_3388e3e48f95/ (stored 0%)\n  adding: tmp/non-multi-bart-large-best-40-lora/runs/Mar13_15-47-05_3388e3e48f95/events.out.tfevents.1710344876.3388e3e48f95.151.0 (deflated 62%)\n  adding: tmp/non-multi-bart-large-best-40-lora/runs/Mar13_15-47-05_3388e3e48f95/events.out.tfevents.1710349369.3388e3e48f95.151.1 (deflated 31%)\n  adding: tmp/non-multi-bart-large-best-40-lora/tokenizer.json (deflated 72%)\n  adding: tmp/non-multi-bart-large-best-40-lora/merges.txt (deflated 53%)\n  adding: tmp/non-multi-bart-large-best-40-lora/training_args.bin (deflated 51%)\n  adding: tmp/non-multi-bart-large-best-40-lora/special_tokens_map.json (deflated 52%)\n  adding: tmp/non-multi-bart-large-best-40-lora/eval_results.json (deflated 44%)\n  adding: tmp/non-multi-bart-large-best-40-lora/tokenizer_config.json (deflated 76%)\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/bart-large-best-40-lora.zip","text/html":"<a href='bart-large-best-40-lora.zip' target='_blank'>bart-large-best-40-lora.zip</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"!rm -rf /kaggle/working/tmp/Einmalumdiewelt-T5-Base_GNAD/non-multi/checkpoint-560 /kaggle/working/tmp/Einmalumdiewelt-T5-Base_GNAD/non-multi/checkpoint-280 /kaggle/working/tmp/Einmalumdiewelt-T5-Base_GNAD/non-multi/checkpoint-420 /kaggle/working/tmp/Einmalumdiewelt-T5-Base_GNAD/non-multi/checkpoint-140","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import the necessary classes\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport html\nimport json\nfrom tqdm import tqdm\nimport pdb\nimport torch\n\nmodel_name = \"t5-base\"\nmodel_path = \"/kaggle/input/tuned-t5-base/checkpoint-24000\"\ntuned = \"tuned\"\ntrainVal = \"train\"\nbatch_size = 50\ninput = '/kaggle/input/task2-sum/{}_4summary.json'.format(trainVal)\noutputFile = './{}_sum_{}_{}.json'.format(trainVal, model_name, tuned)\nwith open(input, 'r') as f:\n    inputJson = json.load(f)\ninput_texts = ['']*len(inputJson[\"data\"])\nfor i in range(len(inputJson[\"data\"])):\n    input_texts[i] = inputJson[\"data\"][i]['article']\n\n# Load the tokenizer and the pre-trained model\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\nmodel.to(device)\n\nfor i in tqdm(range(0,len(inputJson[\"data\"]),batch_size)):\n    # Tokenize the input texts and generate the summaries for the entire batch\n    #print(\"tokenizing..\")\n    inputs = tokenizer(input_texts[i:i+batch_size], return_tensors=\"pt\", max_length=1024, truncation=True, padding=True)\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    #print(\"inferencing..\")\n    summary_ids = model.generate(inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_length=1000)\n    summary_texts = tokenizer.batch_decode(summary_ids, skip_special_tokens=True)\n\n    # Print the generated summaries\n    #print(\"Generated Summaries:\")\n    for j, summary_text in enumerate(summary_texts):\n        inputJson[\"data\"][j+i]['context'] = summary_text\n        # print(f\"Summary {j + 1}:\")\n        # print(summary_text)\nwith open(outputFile, 'w') as f:\n    json.dump(inputJson, f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformer\nmeteor = evaluate.load('meteor')\npredictions = [\"It is a guide to action which ensures that the military always obeys the commands of the party\"]\nreferences = [\"It is a guide to action that ensures that the military will forever heed Party commands\"]\nresults = meteor.compute(predictions=predictions, references=references)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### single text run\n\n# Import the necessary classes\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport html\nimport json\nimport pdb\nfrom tqdm import tqdm\n\nmodel_name = \"t5-base\"\nmodel_path = \"/kaggle/input/tuned-t5-base/checkpoint-24000\"\ntuned = \"tuned\"\ntrainVal = \"val\"\ninput = '/kaggle/input/task2-sum/{}_4summary.json'.format(trainVal)\noutputFile = './output/{}_sum_{}_{}.json'.format(trainVal, model_name, tuned)\nwith open(input, 'r') as f:\n    inputJson = json.load(f)\n    inputJson = inputJson[\"data\"]\n    \n# Load the tokenizer and the pre-trained model\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n    \nprint(\"tokenizing & inferencing..\")\nfor i in tqdm(range(len(inputJson))):\n    input_text = inputJson[i]['article']\n    # Tokenize the input texts and generate the summaries\n    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n    summary_ids = model.generate(inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_length=1000)\n    summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n    inputJson[i]['context'] = summary_text\n\nwith open(outputFile, 'w') as f:\n    json.dump(inputJson, f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}